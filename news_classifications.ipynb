{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from googleapiclient.discovery import build\n",
    "\n",
    "import urllib.request,sys,time\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import pickle\n",
    "from IPython.display import display\n",
    "import sys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "month_mapper = {'jan':'01','feb':'02','mar':'03','apr':'04','may':'05','jun':'06','jul':'07','aug':'08','sep':'09','oct':'10','nov':'11','dec':'12','january':'01','february':'02','march':'03','april':'04','june':'06','july':'07','august':'08','sept':'09','september':'09','october':'10','november':'11','december':'12'}\n",
    "#print(month_mapper)\n",
    "category_mapper = {0: 'religious', 1:'graphics', 2: 'technology',3:'technology',4:'technology', 5:'technology',6:'forsale',7:'automobile',8:'automobile',9:'sport',10:'sport',11:'science',12:'science',13:'science/entertainment',14:'space',15:'religious',16:'politics.guns',\n",
    "    17:'politics',18:'politics',19:'religious'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def customsearchengine(name):\n",
    "    api_key = \"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\" #google api key\n",
    "    resource = build(\"customsearch\", 'v1', developerKey=api_key).cse()\n",
    "    result = resource.list(q=name, cx='xxxxxxxxxxx').execute() #custom search service key\n",
    "    #print(result)\n",
    "    return result\n",
    "def getdict(result):\n",
    "    \n",
    "    temp_dict = dict()\n",
    "\n",
    "    #snippet\n",
    "    #i=0\n",
    "    for item in result['items']:\n",
    "        temp_dict[item['link']] = None\n",
    "    return temp_dict\n",
    "\n",
    "def webscrapping(temp_dict,result):\n",
    "\n",
    "    date_lst=[]\n",
    "    #page =2\n",
    "    #url of the page that we want to Scarpe\n",
    "    #+str() is used to convert int datatype of the page no. and concatenate that to a URL for pagination purposes.\n",
    "    #url = 'https://www.politifact.com/factchecks/list/?page='+str(page)\n",
    "    #i = \"narendra_modi\"\n",
    "    #url = result2['items'][0]['link']\n",
    "    for item in result['items']:\n",
    "        val = ''\n",
    "    #for item in temp_dict.keys():\n",
    "        url = item['link']\n",
    "        ap_date = ''\n",
    "        if re.match(r'.*\\.cnbc\\.com',url):\n",
    "            print(\"url:\",url)\n",
    "            html_content = requests.get(url).text\n",
    "\n",
    "            # Parse the html content\n",
    "            soup = BeautifulSoup(html_content, \"lxml\")\n",
    "            content =  soup.find('div',{'class':'ArticleBody-articleBody'})\n",
    "            #print(content)\n",
    "            if content is None:\n",
    "                \n",
    "                content =  soup.find('div',{'class':'PageBuilder-styles-makeit-col-9--4VOfc PageBuilder-styles-makeit-col--1vq1j PageBuilder-styles-makeit-article--1sqIm'})\n",
    "                if content is None:\n",
    "                    del temp_dict[item['link']]\n",
    "                    continue\n",
    "            #val= ''\n",
    "            for i in content.findAll('div'):\n",
    "                val =val+ '' + i.text\n",
    "\n",
    "                val = re.sub('[^A-Za-z0-9]+', ' ', val)\n",
    "            val = ''.join([i for i in val if not i.isdigit()])\n",
    "            if val == '':\n",
    "                del temp_dict[item['link']]\n",
    "\n",
    "                continue\n",
    "\n",
    "            date =  soup.find('time')\n",
    "            if date is None:\n",
    "                ap_date = 'Not able to extarct the date'\n",
    "                date_lst.append(ap_date)\n",
    "            else:\n",
    "                temp = re.findall(r'.+?Published\\s.+?\\,\\s(\\w+)\\s(\\d+)\\s(\\d+)',str(date))\n",
    "                if temp is not None:\n",
    "                    month = month_mapper[temp[0][0].lower()]\n",
    "                    #print(month)\n",
    "                    ap_date = temp[0][1]+\"-\"+month+\"-\"+temp[0][2]\n",
    "                    #print(ap_date)\n",
    "                    date_lst.append(ap_date)\n",
    "                else:\n",
    "                    ap_date = 'Not able to extarct the date'\n",
    "                    date_lst.append(ap_date)\n",
    "                \n",
    "                #print(temp)\n",
    "\n",
    "                \n",
    "\n",
    "            temp_dict[item['link']] = val\n",
    "        elif re.match(r'.*\\.usatoday\\.com',url):\n",
    "            print(\"url:\",url)\n",
    "            html_content = requests.get(url).text\n",
    "\n",
    "            # Parse the html content\n",
    "            soup = BeautifulSoup(html_content, \"lxml\")\n",
    "            content =  soup.find('main',{'class':'gnt_cw'})\n",
    "            if content is None:\n",
    "                content =  soup.find('div',{'class':'_3YYSt clearfix'})\n",
    "                #print(content)\n",
    "                if content is None:\n",
    "                    content =  soup.find('div',{'class':'article-inner theme-light'})\n",
    "                    if content is None:\n",
    "                        del temp_dict[item['link']]\n",
    "                        continue\n",
    "                    else:\n",
    "                        #val= ''\n",
    "                        for i in content.findAll('p'):\n",
    "                            val =val+ '' + i.text\n",
    "                            val = re.sub('[^A-Za-z0-9]+', ' ', val)\n",
    "                        #print(item['link'])\n",
    "                        #print(val)\n",
    "\n",
    "\n",
    "            \n",
    "            if len(val) == 0:\n",
    "                for i in content.findAll('div'):\n",
    "                    val =val+ '' + i.text\n",
    "\n",
    "                    val = re.sub('[^A-Za-z0-9]+', ' ', val)\n",
    "\n",
    "\n",
    "            val = ''.join([i for i in val if not i.isdigit()])\n",
    "            if len(val) == 0:\n",
    "                del temp_dict[item['link']]\n",
    "                continue\n",
    "\n",
    "            date =  soup.find('div',{'class':'gnt_ar_dt'})\n",
    "            if date is None:\n",
    "                ap_date = 'Not able to extarct the date'\n",
    "                date_lst.append(ap_date)\n",
    "\n",
    "            else:\n",
    "                temp = re.findall(r'.+?Updated\\:.+?\\sET\\s(\\w+)[.]\\s(\\d+)\\,\\s(\\d+)',str(date))\n",
    "                if len(temp) == 0:\n",
    "                    temp = re.findall(r'.+?Updated\\:.+?\\sET\\s(\\w+)\\s(\\d+)\\,\\s(\\d+)',str(date))\n",
    "                    if len(temp)== 0:\n",
    "                        temp = re.findall(r'.+?Published\\s.+?ET\\s(\\w+)[. ]\\s(\\d+)\\,\\s(\\d+)',str(date))\n",
    "                        if len(temp)== 0:\n",
    "                            temp = re.findall(r'.+?Published\\s.+?\\sET\\s(\\w+)\\s(\\d+)\\,\\s(\\d+)',str(date))\n",
    "                            if len(temp) == 0:\n",
    "\n",
    "                                #date_lst.append(\"couldn't extarct date\")\n",
    "                                #continue\n",
    "                                if not date:\n",
    "                                    date =  soup.find('span',{'class':'updated'})\n",
    "                                    #print(date)\n",
    "                                    temp = re.findall(r'.+?(\\d+)\\-(\\d+)\\-(\\d+)',str(date))\n",
    "                                    #print(temp)\n",
    "                                    #ap_date = temp[0][1]+\"-\"+temp[0][0]+\"-\"+temp[0][2]\n",
    "                if len(temp)>0 :\n",
    "                    if temp[0][0].lower() in month_mapper.keys():\n",
    "\n",
    "\n",
    "                        month = month_mapper[temp[0][0].lower()]\n",
    "                        #print(month)\n",
    "                        if len(temp[0][1]) ==1:\n",
    "                            ap_date = \"0\"+temp[0][1]+\"-\"+month+\"-\"+temp[0][2]\n",
    "                        else:\n",
    "                            ap_date = temp[0][1]+\"-\"+month+\"-\"+temp[0][2]\n",
    "                        #print(ap_date)\n",
    "                    else:\n",
    "                        ap_date = temp[0][1]+\"-\"+temp[0][0]+\"-\"+temp[0][2]\n",
    "                        #print(ap_date)\n",
    "                date_lst.append(ap_date)\n",
    "                    #temp_dict[item] = val\n",
    "            #print(item['link'])\n",
    "            #print(val)\n",
    "            temp_dict[item['link']] = val\n",
    "        elif re.match(r'.*\\.politico\\.com',url):\n",
    "            print(\"url:\",url)\n",
    "            html_content = requests.get(url).text\n",
    "\n",
    "            # Parse the html content\n",
    "            soup = BeautifulSoup(html_content, \"lxml\")\n",
    "            content =  soup.find('div',{'class':'page-content page-content--story story-type--core'})\n",
    "            if content is None:\n",
    "                content =  soup.find('div',{'class':'story-text'})\n",
    "                if content is None:\n",
    "                    del temp_dict[item['link']]\n",
    "                    continue\n",
    "\n",
    "            #val= ''\n",
    "            for i in content.findAll('div'):\n",
    "                val =val+ '' + i.text\n",
    "\n",
    "                val = re.sub('[^A-Za-z0-9]+', ' ', val)\n",
    "            val = ''.join([i for i in val if not i.isdigit()])\n",
    "            if val == '':\n",
    "                del temp_dict[item['link']]\n",
    "\n",
    "                continue\n",
    "            date =  soup.find('p',{'class':'story-meta__updated'})\n",
    "            if date is not None:\n",
    "                temp = re.findall(r'.+?Updated.+?(\\d+)\\-(\\d+)-(\\d+)',str(date))\n",
    "                if temp is not None:\n",
    "                    ap_date = temp[0][2]+\"-\"+temp[0][1]+\"-\"+temp[0][0]\n",
    "                    date_lst.append(ap_date)\n",
    "                else:\n",
    "                    ap_date = 'Not able to extarct the date'\n",
    "                    date_lst.append(ap_date)\n",
    "\n",
    "            else:\n",
    "                date =  soup.find('p',{'class':'story-meta__timestamp'})\n",
    "                #print(date)\n",
    "                if date is not None:\n",
    "                    temp = re.findall(r'.+?(\\d+)\\-(\\d+)\\-(\\d+).+?',str(date))\n",
    "                    if temp is not None:\n",
    "                        ap_date = temp[0][1]+\"-\"+temp[0][0]+\"-\"+temp[0][2]\n",
    "                        date_lst.append(ap_date)\n",
    "                    else:\n",
    "                        ap_date = 'Not able to extarct the date'\n",
    "                        date_lst.append(ap_date)\n",
    "                else:\n",
    "                    date =  soup.find('time')\n",
    "                    #print()\n",
    "                    #temp = re.findall(r'.+?\\>(\\d+)\\/(\\d+)\\/(\\d+)',str(date))\n",
    "                    temp = re.findall(r'.+?(\\d+)\\-(\\d+)\\-(\\d+).+?',str(date))\n",
    "                    #print(temp)\n",
    "                    if temp is not None:\n",
    "                        ap_date = temp[0][1]+\"-\"+temp[0][0]+\"-\"+temp[0][2]\n",
    "                        date_lst.append(ap_date)\n",
    "                    else:\n",
    "                        ap_date = 'Not able to extarct the date'\n",
    "                        date_lst.append(ap_date)\n",
    "            temp_dict[item['link']] = val\n",
    "        elif re.match(r'.*\\.bbc\\.com',url):\n",
    "            print(\"url:\",url)\n",
    "            html_content = requests.get(url).text\n",
    "\n",
    "            # Parse the html content\n",
    "            soup = BeautifulSoup(html_content, \"lxml\")\n",
    "            content =  soup.find('div',{'class':'ssrcss-181c4hk-SectionWrapper eustbbg3'})\n",
    "            if content is None:\n",
    "                del temp_dict[item['link']]\n",
    "                continue\n",
    "            #val= ''\n",
    "            for i in content.findAll('div'):\n",
    "                val =val+ '' + i.text\n",
    "\n",
    "                val = re.sub('[^A-Za-z0-9]+', ' ', val)\n",
    "            val = ''.join([i for i in val if not i.isdigit()])\n",
    "            if val == '':\n",
    "                del temp_dict[item['link']]\n",
    "\n",
    "                continue\n",
    "\n",
    "            date =  soup.find('time')\n",
    "            if date is None:\n",
    "                ap_date = 'Not able to extarct the date'\n",
    "                date_lst.append(ap_date)\n",
    "            else:\n",
    "                temp = re.findall(r'datetime\\=\\\"([0-9]+)\\-([0-9]+)\\-([0-9]+).*',str(date))\n",
    "                if temp is not None:\n",
    "                    ap_date = temp[0][2]+\"-\"+temp[0][1]+\"-\"+temp[0][0]\n",
    "                    date_lst.append(ap_date)\n",
    "                else:\n",
    "                    ap_date = 'Not able to extarct the date'\n",
    "                    date_lst.append(ap_date)\n",
    "                #ap_date = temp[0][2]+\"-\"+temp[0][1]+\"-\"+temp[0][0]\n",
    "                #date_lst.append(ap_date)\n",
    "            temp_dict[item['link']] = val\n",
    "        elif re.match(r'.*\\.cnn\\.com',url):\n",
    "            print(\"url:\",url)\n",
    "            html_content = requests.get(url).text\n",
    "\n",
    "            # Parse the html content\n",
    "            soup = BeautifulSoup(html_content, \"lxml\")\n",
    "            content =  soup.find('div',{'class':'l-container'})\n",
    "            #print(content)\n",
    "            if content is None:\n",
    "                del temp_dict[item['link']]\n",
    "                continue\n",
    "\n",
    "            #val= ''\n",
    "            for i in content.findAll('div'):\n",
    "                val =val+ '' + i.text\n",
    "\n",
    "                val = re.sub('[^A-Za-z0-9]+', ' ', val)\n",
    "            val = ''.join([i for i in val if not i.isdigit()])\n",
    "            if val == '':\n",
    "                del temp_dict[item['link']]\n",
    "\n",
    "                continue\n",
    "\n",
    "            date =  soup.find('p',{'class':'update-time'})\n",
    "            if date is None:\n",
    "                ap_date = 'Not able to extarct the date'\n",
    "                date_lst.append(ap_date)\n",
    "            else:\n",
    "                #print(date)\n",
    "                temp = re.findall(r'\\s(\\w+)\\s([0-9]+),\\s([0-9]+).*',str(date))\n",
    "                if temp is not None:\n",
    "                    month = month_mapper[temp[0][0].lower()]\n",
    "                    #print(month)\n",
    "                    ap_date = temp[0][1]+\"-\"+month+\"-\"+temp[0][2]\n",
    "                    ap_date\n",
    "                    date_lst.append(ap_date)\n",
    "                else:\n",
    "                    ap_date = 'Not able to extarct the date'\n",
    "                    date_lst.append(ap_date)\n",
    "                #print(temp)\n",
    "                \n",
    "\n",
    "            temp_dict[item['link']] = val\n",
    "\n",
    "    return temp_dict, date_lst\n",
    "    #temp_dict\n",
    "def getdataframe(temp_dict,date_lst):\n",
    "    idx = ['Links','Data']\n",
    "    rona = pd.DataFrame.from_dict(temp_dict, orient = 'index')\n",
    "    rona = rona.reset_index()\n",
    "    rona= rona.rename(columns = {'index':'Links',0:'Data'})\n",
    "    #rona['Date Published'] = date_lst\n",
    "    return rona\n",
    "\n",
    "def getmodel():\n",
    "    #to import the saved model\n",
    "    with open(r\".\\svm_v1.pkl\", 'rb') as file:  \n",
    "        text_clf_svm = pickle.load(file)\n",
    "    return text_clf_svm\n",
    "\n",
    "def getfinaldf(rona):\n",
    "    text_clf_svm = getmodel()\n",
    "    predict = text_clf_svm.predict(rona['Data'].to_list())\n",
    "    rona['Category'] = predict\n",
    "    rona['Category'] = rona['Category'].apply(lambda x: category_mapper[x])\n",
    "    return rona\n",
    "\n",
    "def getfinalresult(rona):\n",
    "    catper=[]\n",
    "    totallen = len(rona.index)\n",
    "    tempStore = rona['Category'].unique()\n",
    "    #print(tempStore)\n",
    "    for i in tempStore:\n",
    "        temp_df = rona[rona['Category'] == i]\n",
    "        totalpol = len(temp_df)\n",
    "        perpol = int(totalpol/totallen*100)\n",
    "        #print(perpol)\n",
    "        catper.append(perpol)\n",
    "    return tempStore,catper\n",
    "\n",
    "def getdata(name):\n",
    "    result = customsearchengine(name)\n",
    "    #print(result)\n",
    "    if result['searchInformation']['totalResults'] == '0':\n",
    "        #print(\"Unable to fetch any news articles related to \"+name)\n",
    "        sys.exit(\"Unable to fetch any news articles related to \"+name)\n",
    "        #exit\n",
    "    temp_dict = getdict(result)\n",
    "    temp_dict,date_lst = webscrapping(temp_dict,result)\n",
    "    rona =getdataframe(temp_dict,date_lst)\n",
    "    final_df = getfinaldf(rona)\n",
    "    display(rona)\n",
    "    tempStore,catper = getfinalresult(rona)\n",
    "    \n",
    "    print(\"Category Exposure:\")\n",
    "    for i in range(len(tempStore)):\n",
    "        print(str(i+1)+'.'+tempStore[i]+':'+str(catper[i])+'%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = input(\"Enter the applicant's name:\")\n",
    "getdata(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
